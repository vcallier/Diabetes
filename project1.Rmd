---
title: 'Project: Predicting Diabetes in Pima Indians'
author: "Viviane Callier"
date: "1/23/2021"
output:
  pdf_document: default
---

General Procedure: 
1. Investigate the data
2. Specify candidate model
3. Estimate the model parameters
4. Assess the fitted model
5. Select informative predictors 

```{r}
#loading the data
library(tidyverse)
library(boot)
library(glmnet)
library(olsrr)
library(tree)

#diabetes <- read_csv("/Users/vc12/Documents/UTSA statistics/DataMining2-spring2021/project diabetes prediction/diabetes.csv")


diabetes <- read_csv("/Users/vivianecallier/Documents/UTSA statistics/DataMining2-spring2021/project diabetes prediction/diabetes.csv")

```

Visualizing the data. 
```{r}
ggplot(data = diabetes) +
  geom_point (mapping = aes(x = BloodPressure, y = Outcome))

```



#Parametric Approach: Logistic Regression
As a first proposal, let's try using a logistic regression to predict whether a person will have diabetes or not based on the available predictor variables. 

```{r}
glm.fits = glm(Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age, data = diabetes, family = binomial)
summary(glm.fits)
```



Now, we want to use the predict function to predict the probability that an individual will have diabetes, given values of the predictors.
```{r}
#predicting whether an individual will have diabetes given the values of the predictors
glm.probs = predict(glm.fits, type = "response")

#creating a confusion matrix
glm.pred = rep("0", nrow(diabetes)) # 0 is healthy
glm.pred[glm.probs > 0.5] = "1"     # 1 is diabetic
table(glm.pred, diabetes$Outcome)

#calculating the fraction of days for which the prediction was correct
mean(glm.pred == diabetes$Outcome)
mean(glm.pred != diabetes$Outcome)
```

In this case, the logistic regression correctly predicted the health status (diabetic or healthy) 78 percent of the time. That means that the training error rate is about 22 percent. However, the training error rate is overly optimistic and tends to underestimate the test error rate. To better assess the accuracy of the model, we want to use cross-validation. 

```{r}

error = c()

for (i in 1:1000){
  
#splitting into training and validation sets
set.seed(i)
train = sample(768, 384)
diabetes.train = diabetes[train,]
diabetes.test = diabetes[-train,]

#training model on the training set
glm.fits1 = glm(Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age, data = diabetes.train, family = binomial)

#test error rate
glm.probs.test = predict(glm.fits1, diabetes.test, type = "response")

#creating a confusion matrix
glm.pred.test = rep("0", 384)            # 0 is healthy
glm.pred.test[glm.probs.test > 0.5] = "1"     # 1 is diabetic
table(glm.pred.test, diabetes.test$Outcome)

#calculating the fraction of days for which the prediction was correct
error[i] = mean(glm.pred.test != diabetes.train$Outcome)

}

```


```{r}
hist(error, breaks = 50)
summary(error)
```

When we do a cross validation by splitting the data into test and validation sets, the model accurately predicts diabetic status only about 57 percent of the time. The mean test error rate is 43.29 percent. 



```{r}
#cross validation error using LOOCV
cv.err = cv.glm(diabetes, glm.fits)
cv.err$delta

```
The LOOCV error rate is 15.7 percent. 

Let's also try the K-fold cross-validation. 

```{r}
# K fold cross validation
cv.error.5 = cv.glm(diabetes, glm.fits, K=5)$delta
cv.error.5
```

The K-fold cross-validation error with K=5 is 15.8 percent. 





#Non-parametric approach: KNN classification
Let's try a non-parametric approach, KNN classification. 

First, we need to standardize all of the data so all variables have a mean of zero and a standard deviation of 1. 
```{r}
library(class)
standardized.X = scale(diabetes[,-9])


#now splitting into a test set and training set. 
# doing two for loops. 
#The inner loop (i) is sampling the test and training sets 100 different ways, and then calculates the prediction error rate. # The outer loop (j) is increasing the number of K neighbors from 2 to 40. 
error = tibble(V1 = double())

# j is the number of K neighbors. 
for (j in 2:40){
  
  # i is the seed for sampling the test and training sets. 
  for (i in 1:100){
  set.seed(i)  
  test = sample(768, 384)
  train.X = standardized.X[-test,]
  test.X = standardized.X[test,]

  train.Y = diabetes$Outcome[-test]
  test.Y = diabetes$Outcome[test]

  knn.pred = knn(train.X, test.X, train.Y, k=j)
  error[j,i] = mean(test.Y != knn.pred)
  }
  
}
```

```{r}
error %>% drop_na 


```




```{r}

plot(mean.error)
min(mean.error[2:length(mean.error)])
match(min(mean.error[2:length(mean.error)]), mean.error)
```

The optimal number of neighbors to minimize the test prediction error is around 20. The test prediction error decreases as K increases from 2 to 19, and then it increases again as K increases from 23 to 50. 
The minimum prediction error rate is 24.21 percent, and it is attained at K = 19 and K = 22.


#Decision Trees. 

Let's try a classification tree. 

```{r}
Outcome1 = ifelse(diabetes$Outcome >= 1,  "Diabetic", "Healthy")
diabetes = data.frame(diabetes, Outcome1)

tree.diabetes = tree(Outcome1 ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age, data = diabetes, method = "class")
summary(tree.diabetes)

```
The missclassification error rate is 20.57 percent. 

```{r}
plot(tree.diabetes)
text(tree.diabetes, pretty = 0)
```

Estimating the test error of the classification tree. 

```{r}
set.seed(1)
train = sample(768, 384)
diabetes.train = diabetes[train,]
diabetes.test = diabetes[-train,]

#training model on the training set
tree.diabetes1 = tree(Outcome1 ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age, data = diabetes.train, method = "class")

#test error rate
tree.pred = predict(tree.diabetes1, diabetes.test, type = "class")

#creating a confusion matrix
table(tree.pred, diabetes.test$Outcome1)


```

Cross validation. 
```{r}
set.seed(2)
cv.diabetes.tree = cv.tree(tree.diabetes1, FUN = prune.misclass)
cv.diabetes.tree

```

dev corresponds to the cross-validation error rate. 
size represents the number of terminal nodes of each tree considered.
k is the value of the cost-complexity parameter used. 


```{r}
plot(cv.diabetes.tree$size, cv.diabetes.tree$dev, type = "b")
plot(cv.diabetes.tree$k, cv.diabetes.tree$dev, type = "b")
```

The tree size of 20 terminal nodes results in the lowest cross-validation error rate. 

Let's make a pruned tree and see how well it performs on the test dataset. 
```{r}

prune.diabetes = prune.misclass(tree.diabetes1, best = 20)
plot(prune.diabetes)
text(prune.diabetes, pretty = 0)

tree.pred =predict(prune.diabetes, diabetes.test, type = "class")
table(tree.pred, diabetes.test$Outcome1)
```
```{r}
#correct classification rate
(80+215)/(80+215+35+54)

#error rate
1-(80+215)/(80+215+35+54)
```

ABout 76.82 percent of observations are correctly classified. The classification error rate is 23.18 percent. 


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
